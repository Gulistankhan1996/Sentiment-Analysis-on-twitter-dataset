{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcSCU1FARi-T",
        "outputId": "2a11be54-40d7-4936-8555-743b9f573d32"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "I_ES0RlPRg9E"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Twitter Sentiment Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.ml.feature import Word2Vec\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV data into a Spark DataFrame\n",
        "df = spark.read.csv(\"/content/train_data.csv\", header=True, inferSchema=True)\n",
        "df.show(20)"
      ],
      "metadata": {
        "id": "ZR3wex1aS6GJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8859bc9a-db59-4efe-88a6-9278892164a1"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------------------+\n",
            "| id|label|               tweet|\n",
            "+---+-----+--------------------+\n",
            "|  1|    0| @user when a fat...|\n",
            "|  2|    0|@user @user thank...|\n",
            "|  3|    0|  bihday your maj...|\n",
            "|  4|    0|#model   i love u...|\n",
            "|  5|    0| factsguide: soci...|\n",
            "|  6|    0|[2/2] huge fan fa...|\n",
            "|  7|    0| @user camping to...|\n",
            "|  8|    0|the next school y...|\n",
            "|  9|    0|we won!!! love th...|\n",
            "| 10|    0| @user @user welc...|\n",
            "| 11|    0| â #ireland con...|\n",
            "| 12|    0|we are so selfish...|\n",
            "| 13|    0|i get to see my d...|\n",
            "| 14|    1|@user #cnn calls ...|\n",
            "| 15|    1|no comment!  in #...|\n",
            "| 16|    0|ouch...junior is ...|\n",
            "| 17|    0|i am thankful for...|\n",
            "| 18|    1|retweet if you ag...|\n",
            "| 19|    0|its #friday! ð...|\n",
            "| 20|    0|as we all know, e...|\n",
            "+---+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "# Lowercase the text if not already done\n",
        "if \"tweet\" in df.columns:\n",
        "    df = df.withColumn(\"tweet\", lower(col(\"tweet\")))\n",
        "\n",
        "# Remove punctuation and special characters if not already done\n",
        "if \"tweet\" in df.columns:\n",
        "    df = df.withColumn(\"tweet\", regexp_replace(col(\"tweet\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
        "\n",
        "# Tokenize text if not already tokenized\n",
        "if \"words\" not in df.columns:\n",
        "    tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
        "    df = tokenizer.transform(df)\n",
        "\n",
        "# Remove stopwords if not already removed\n",
        "if \"filtered\" not in df.columns:\n",
        "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "    df = remover.transform(df)\n",
        "\n",
        "df.show(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoiXSDSSUd89",
        "outputId": "8899d939-74e3-451d-f0ca-fbfdd8775182"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------------------+--------------------+--------------------+\n",
            "| id|label|               tweet|               words|            filtered|\n",
            "+---+-----+--------------------+--------------------+--------------------+\n",
            "|  1|    0| user when a fath...|[, user, when, a,...|[, user, father, ...|\n",
            "|  2|    0|user user thanks ...|[user, user, than...|[user, user, than...|\n",
            "|  3|    0|  bihday your maj...|[, , bihday, your...|[, , bihday, maje...|\n",
            "|  4|    0|model   i love u ...|[model, , , i, lo...|[model, , , love,...|\n",
            "|  5|    0| factsguide socie...|[, factsguide, so...|[, factsguide, so...|\n",
            "|  6|    0|22 huge fan fare ...|[22, huge, fan, f...|[22, huge, fan, f...|\n",
            "|  7|    0| user camping tom...|[, user, camping,...|[, user, camping,...|\n",
            "|  8|    0|the next school y...|[the, next, schoo...|[next, school, ye...|\n",
            "|  9|    0|we won love the l...|[we, won, love, t...|[won, love, land,...|\n",
            "| 10|    0| user user welcom...|[, user, user, we...|[, user, user, we...|\n",
            "| 11|    0|  ireland consume...|[, , ireland, con...|[, , ireland, con...|\n",
            "| 12|    0|we are so selfish...|[we, are, so, sel...|[selfish, orlando...|\n",
            "| 13|    0|i get to see my d...|[i, get, to, see,...|[get, see, daddy,...|\n",
            "| 14|    1|user cnn calls mi...|[user, cnn, calls...|[user, cnn, calls...|\n",
            "| 15|    1|no comment  in au...|[no, comment, , i...|[comment, , austr...|\n",
            "| 16|    0|ouchjunior is ang...|[ouchjunior, is, ...|[ouchjunior, angr...|\n",
            "| 17|    0|i am thankful for...|[i, am, thankful,...|[thankful, paner,...|\n",
            "| 18|    1|retweet if you ag...|[retweet, if, you...|    [retweet, agree]|\n",
            "| 19|    0|its friday  smile...|[its, friday, , s...|[friday, , smiles...|\n",
            "| 20|    0|as we all know es...|[as, we, all, kno...|[know, essential,...|\n",
            "+---+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **word2vec**"
      ],
      "metadata": {
        "id": "GZQU_Mmka_tW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "word2vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"filtered\", outputCol=\"features\")\n",
        "model = word2vec.fit(df)\n",
        "df = model.transform(df)"
      ],
      "metadata": {
        "id": "qCgLxKynUhPG"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85MQcxRL3guY",
        "outputId": "ab63f08e-8dd8-48fe-d279-8afae0e660e2"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "| id|label|               tweet|               words|            filtered|            features|\n",
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|  1|    0| user when a fath...|[, user, when, a,...|[, user, father, ...|(1000,[0,1,164,16...|\n",
            "|  2|    0|user user thanks ...|[user, user, than...|[user, user, than...|(1000,[0,1,19,22,...|\n",
            "|  3|    0|  bihday your maj...|[, , bihday, your...|[, , bihday, maje...|(1000,[0,17],[2.0...|\n",
            "|  4|    0|model   i love u ...|[model, , , i, lo...|[model, , , love,...|(1000,[0,2,7,8,27...|\n",
            "|  5|    0| factsguide socie...|[, factsguide, so...|[, factsguide, so...|(1000,[0,199],[4....|\n",
            "|  6|    0|22 huge fan fare ...|[22, huge, fan, f...|[22, huge, fan, f...|(1000,[15,109,172...|\n",
            "|  7|    0| user camping tom...|[, user, camping,...|[, user, camping,...|(1000,[0,1,57],[1...|\n",
            "|  8|    0|the next school y...|[the, next, schoo...|[next, school, ye...|(1000,[0,19,64,77...|\n",
            "|  9|    0|we won love the l...|[we, won, love, t...|[won, love, land,...|(1000,[2,754,998]...|\n",
            "| 10|    0| user user welcom...|[, user, user, we...|[, user, user, we...|(1000,[0,1,6,622]...|\n",
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic regression"
      ],
      "metadata": {
        "id": "GB94nU0VbM8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Convert label to numerical format\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label_index\",threshold=0.3)\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, lr])\n",
        "\n",
        "# Split data into training and test sets\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=12345)\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train)\n"
      ],
      "metadata": {
        "id": "L0xeDxbkUuy5"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "print(f\"Test F1 Score = {f1_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcSpgEy7U3KW",
        "outputId": "402b99af-1301-43ba-bae8-678477b245f7"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1 Score = 0.9328921019120989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "T9mBs2Q7bRTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Convert label to numerical format\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label_index\")\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, svm])\n",
        "\n",
        "# Split data into training and test sets\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=12345)\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train)"
      ],
      "metadata": {
        "id": "ENi0y_3xaMO_"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "print(f\"Test F1 Score = {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eICSDFahaJc2",
        "outputId": "2b0d66b4-c415-432c-865b-80cda940a7a8"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1 Score = 0.907774579566599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8qEkaTQoVeg"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zI8bE1SkoVMf"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TF IDF**"
      ],
      "metadata": {
        "id": "Lc_GJIU0bVyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g15J7cM7bVYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction using TF-IDF\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
        "featurizedData = hashingTF.transform(df)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidfFeatures\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n"
      ],
      "metadata": {
        "id": "lMA5VopvVEBs"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmKylH7y4fP-",
        "outputId": "9d1b0df5-5020-410d-a3c5-a2d54947b167"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "| id|label|               tweet|               words|            filtered|            features|\n",
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|  1|    0| user when a fath...|[, user, when, a,...|[, user, father, ...|(1000,[0,1,164,16...|\n",
            "|  2|    0|user user thanks ...|[user, user, than...|[user, user, than...|(1000,[0,1,19,22,...|\n",
            "|  3|    0|  bihday your maj...|[, , bihday, your...|[, , bihday, maje...|(1000,[0,17],[2.0...|\n",
            "|  4|    0|model   i love u ...|[model, , , i, lo...|[model, , , love,...|(1000,[0,2,7,8,27...|\n",
            "|  5|    0| factsguide socie...|[, factsguide, so...|[, factsguide, so...|(1000,[0,199],[4....|\n",
            "|  6|    0|22 huge fan fare ...|[22, huge, fan, f...|[22, huge, fan, f...|(1000,[15,109,172...|\n",
            "|  7|    0| user camping tom...|[, user, camping,...|[, user, camping,...|(1000,[0,1,57],[1...|\n",
            "|  8|    0|the next school y...|[the, next, schoo...|[next, school, ye...|(1000,[0,19,64,77...|\n",
            "|  9|    0|we won love the l...|[we, won, love, t...|[won, love, land,...|(1000,[2,754,998]...|\n",
            "| 10|    0| user user welcom...|[, user, user, we...|[, user, user, we...|(1000,[0,1,6,622]...|\n",
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic regression"
      ],
      "metadata": {
        "id": "09Kfdyv9baOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Convert label to numerical format\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label_index\",threshold=0.3)\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, lr])\n",
        "\n",
        "# Split data into training and test sets\n",
        "train, test = rescaledData.randomSplit([0.8, 0.2], seed=12345)\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train)\n"
      ],
      "metadata": {
        "id": "hoZr-4wXVdRa"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "print(f\"Test F1 Score = {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8UwRH-3WM4Y",
        "outputId": "9129f907-43bf-49d0-dcf4-232de720d1f4"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1 Score = 0.9362110670558152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "FMjWD0wPbe6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Convert label to numerical format\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label_index\")\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, svm])\n",
        "\n",
        "# Split data into training and test sets\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=12345)\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train)\n"
      ],
      "metadata": {
        "id": "lPl6wrAIWT8G"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "print(f\"Test F1 Score = {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0C0gtgoZ2YJ",
        "outputId": "325d1d63-26ec-4f5b-c17d-2cf694aacf94"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1 Score = 0.907774579566599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag of Words**"
      ],
      "metadata": {
        "id": "DQzjpvSbvR49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Assuming `df` is your DataFrame containing the data\n",
        "\n",
        "# Drop the existing \"features\" column if it exists\n",
        "if \"features\" in df.columns:\n",
        "    df = df.drop(\"features\")\n",
        "\n",
        "# Convert text data into numerical features using Bag of Words (CountVectorizer)\n",
        "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=1000)  # Adjust vocabSize as needed\n",
        "model = cv.fit(df)\n",
        "df = model.transform(df)"
      ],
      "metadata": {
        "id": "3QlYL231aB6A"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND8vMvEX4pNL",
        "outputId": "53358674-716d-4c27-8946-9f0f1e585cfe"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "| id|label|               tweet|               words|            filtered|            features|\n",
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|  1|    0| user when a fath...|[, user, when, a,...|[, user, father, ...|(1000,[0,1,164,16...|\n",
            "|  2|    0|user user thanks ...|[user, user, than...|[user, user, than...|(1000,[0,1,19,22,...|\n",
            "|  3|    0|  bihday your maj...|[, , bihday, your...|[, , bihday, maje...|(1000,[0,17],[2.0...|\n",
            "|  4|    0|model   i love u ...|[model, , , i, lo...|[model, , , love,...|(1000,[0,2,7,8,27...|\n",
            "|  5|    0| factsguide socie...|[, factsguide, so...|[, factsguide, so...|(1000,[0,199],[4....|\n",
            "|  6|    0|22 huge fan fare ...|[22, huge, fan, f...|[22, huge, fan, f...|(1000,[15,109,172...|\n",
            "|  7|    0| user camping tom...|[, user, camping,...|[, user, camping,...|(1000,[0,1,57],[1...|\n",
            "|  8|    0|the next school y...|[the, next, schoo...|[next, school, ye...|(1000,[0,19,64,77...|\n",
            "|  9|    0|we won love the l...|[we, won, love, t...|[won, love, land,...|(1000,[2,754,998]...|\n",
            "| 10|    0| user user welcom...|[, user, user, we...|[, user, user, we...|(1000,[0,1,6,622]...|\n",
            "+---+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic regression"
      ],
      "metadata": {
        "id": "28up_4JJw27W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert label to numerical format\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "\n",
        "# Logistic Regression classifier\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label_index\")\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, lr])\n",
        "\n",
        "# Split data into training and test sets\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=12345)\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train)\n"
      ],
      "metadata": {
        "id": "Zv06TJA3lBJm"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# # Check if \"features\" column exists in test dataset and drop it if it does\n",
        "# if \"features\" in test.columns:\n",
        "#     test = test.drop(\"features\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "\n",
        "# Print the F1 score\n",
        "print(f\"Test F1 Score = {f1_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR78oroMvhbX",
        "outputId": "c8f74705-e807-4b4c-d949-0bde0fb4c247"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1 Score = 0.9406806479085082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Convert label to numerical format\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label_index\")\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, svm])\n",
        "\n",
        "# Split data into training and test sets\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=12345)\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train)"
      ],
      "metadata": {
        "id": "NwkVZqnpvh5K"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "print(f\"Test F1 Score = {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFH9NPaexBL1",
        "outputId": "3fe08de2-d7c3-4c85-f2e4-fe40f72244de"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1 Score = 0.9430677831262497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6U8ZK37-xJVG"
      },
      "execution_count": 144,
      "outputs": []
    }
  ]
}